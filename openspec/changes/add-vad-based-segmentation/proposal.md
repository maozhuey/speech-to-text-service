# 实现基于声音间隔的智能断句

## 变更概述

实现基于语音活动检测（VAD）的智能断句功能，根据实际说话停来自动判断句子边界，替代当前固定时长的断句策略。

## Why

当前系统使用固定时长（5秒）进行断句，存在以下问题：

1. **断句不准确**：在句子中间强制断句，导致语义不完整
2. **用户体验差**：说话流畅时被频繁打断，或停顿时继续等待
3. **效率低下**：无论是否说话都在累积音频，浪费计算资源

### 实际问题示例
- 用户说："今天天气很好，我想去公园散步，但是可能要下雨"
- 当前系统可能在"很好"或"散步"后强制断句
- 理想情况：检测到用户停顿（如800ms静音）后再断句

## 目标

1. **智能断句**：基于VAD检测静音片段，在自然停顿处断句
2. **超时保护**：防止长时间持续说话不断句（最大20秒）
3. **即时响应**：检测到明显停顿后立即处理，无需等待固定时长
4. **向后兼容**：保留固定时长断句作为备选方案

## 范围

### 包含
- 在WebSocket音频处理器中集成VAD检测
- 实现基于静音检测的断句逻辑
- 添加可配置的断句参数（静音阈值、超时时长等）
- 前端显示断句状态（检测中、处理中）
- 添加配置项到环境变量

### 不包含
- 说话人分离优化
- 实时字幕显示优化
- 音频质量改进
- 新的UI组件

## 技术方案

### 核心逻辑
1. **实时VAD检测**：每个音频块通过VAD判断是否包含语音
2. **静音计数**：连续检测到静音时累加时长
3. **触发条件**：
   - 条件A：连续静音超过阈值（默认800ms）→ 立即断句
   - 条件B：累积音频超过最大时长（默认20秒）→ 强制断句
   - 条件C：停止录音信号 → 处理剩余音频

### VAD使用
- 使用现有FunASR的VAD pipeline（`speech_fsmn_vad_zh-cn-16k-common-pytorch`）
- 每个音频块（4096采样，约256ms）调用一次VAD
- 记录语音/静音状态用于断句判断

## 风险和依赖

### 风险
1. **VAD性能**：频繁调用VAD可能增加CPU使用
2. **误判风险**：环境噪音可能导致VAD误判为语音
3. **延迟增加**：VAD检测可能增加处理延迟

### 缓解措施
- 使用轻量级VAD模型
- 添加可配置的阈值
- 监控性能指标

### 依赖
1. 现有FunASR VAD pipeline正常工作
2. WebSocket连接稳定
3. 音频数据格式正确（16kHz PCM）

## 验收标准

1. **功能验收**
   - 用户说话停顿约800ms后自动断句
   - 连续说话20秒后强制断句
   - 停止录音时处理所有剩余音频

2. **性能验收**
   - VAD检测延迟 < 100ms
   - 整体识别延迟保持在2秒以内
   - CPU使用率增加 < 20%

3. **用户体验**
   - 断句位置符合自然语言习惯
   - 减少句子中间断句的情况
   - 响应及时，无明显卡顿

## 配置参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `VAD_SILENCE_THRESHOLD` | 800 | 连续静音时长（ms）触发断句 |
| `VAD_MAX_SEGMENT_DURATION` | 20000 | 单段最大时长（ms），防止过长不断句 |
| `VAD_CHUNK_SIZE` | 4096 | VAD检测块大小（采样点数） |
| `VAD_ENABLE` | true | 是否启用VAD断句，false则回退到固定时长 |

## 后续优化

- 评估webrtcvad作为替代方案（更快但可能精度较低）
- 添加说话人分离辅助断句
- 基于语义的断句优化（NLP模型）
